{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxim-chn/deep-learning/blob/maxim/src/preprocessor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "\n",
        "*   [Dependencies management](#dependecies-management)\n",
        "*   [Project Config Init](#project-config-init)\n",
        "*   [Connection Setup to Google Cloud Storage](#connection-setup-to-cloud-storage)\n",
        "*   [Task Definition](#task-definition)\n",
        "*   [Batch Definition](#batch-definition)\n",
        "*   [Main Execution Path](#main-execution-path)"
      ],
      "metadata": {
        "id": "C7fCMvosxdLx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3LWaJrauRx-"
      },
      "source": [
        "## Dependencies Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "04UJ1tsEuRx_"
      },
      "outputs": [],
      "source": [
        "# install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "HinmFeSBuRyA"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "import json\n",
        "import multiprocessing\n",
        "import os\n",
        "from time import sleep\n",
        "import requests\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.cloud import storage as storage\n",
        "from google.colab import auth as auth\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Back to Table of Contents](#table-of-contents)"
      ],
      "metadata": {
        "id": "oHHkOpmcC9TR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Config Init\n",
        "We initiate configuration like location of buckets for unpreprocessed and processed images, machine limit in terms of parallel tasks, runtime parameters that define types of tasks etc."
      ],
      "metadata": {
        "id": "wVHsMmRIRvAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FLOW = \"classic\" # Possibilities: \"classic\", \"noise_framed\"\n",
        "VERSION = \"maxim\"\n",
        "CONFIG_URL = f\"https://raw.githubusercontent.com/maxim-chn/deep-learning/{VERSION}/config/project.json\""
      ],
      "metadata": {
        "id": "dkKgDxaSby31"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(CONFIG_URL)\n",
        "if response.status_code == 200:\n",
        "  project_config = response.json()\n",
        "else:\n",
        "  raise RuntimeError(\"Failed to download project config\")\n",
        "project_config[\"machine\"][\"tasks_number\"] = multiprocessing.cpu_count()"
      ],
      "metadata": {
        "id": "L5s8l5W0VGf1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKETS = project_config[\"buckets\"]\n",
        "MACHINE = project_config[\"machine\"]\n",
        "MODEL_INPUT = project_config[\"model_input\"]\n",
        "PROJECT_ID = project_config[\"id\"]"
      ],
      "metadata": {
        "id": "gE5VsFMwpNiw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Back to Table of Contents](#table-of-contents)"
      ],
      "metadata": {
        "id": "W134JB14rxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connection Setup to Google Cloud Storage\n",
        "Our goal is to gain access to the *Google Cloud Filesystem*, which is [Google Buckets](https://console.cloud.google.com/storage/browser?hl=en&project=confident-trail-426114-e6) in the case of our project.\n",
        "\n",
        "The interaction with the Google Cloud Filesystem is managed by the [google-cloud-storage module](https://cloud.google.com/python/docs/reference/storage/latest), initialized with the [google-cloud-storage's Client](https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.client.Client)"
      ],
      "metadata": {
        "id": "qk5kO8sxCHxf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U9Oiv1o2uRyA"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "client = storage.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Back to Table of Contents](#table-of-contents)"
      ],
      "metadata": {
        "id": "HYng-JnCr8oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom API Definition\n",
        "This is utility methods and parameters defined specifically for the purpose of preprocessing."
      ],
      "metadata": {
        "id": "jgEadx6cnrOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image API"
      ],
      "metadata": {
        "id": "qLw8YFGzn_lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProcessedImage:\n",
        "  num_of_zeros_in_name_prefix = 4\n",
        "\n",
        "  def __init__(self, counter: int, tr_data: list = [], te_data: bytes = None):\n",
        "    self._counter = counter\n",
        "    self._counted_tr = 0\n",
        "    self._init_name()\n",
        "    self._init_tr_data(tr_data)\n",
        "    self._init_te_data(te_data)\n",
        "\n",
        "  @property\n",
        "  def name(self) -> str:\n",
        "    return self._name\n",
        "\n",
        "  @property\n",
        "  def te_data(self) -> dict:\n",
        "    return self._te_data\n",
        "\n",
        "  @property\n",
        "  def tr_data(self) -> list:\n",
        "    return self._tr_data\n",
        "\n",
        "  def _init_name(self) -> None:\n",
        "    if self._counter in [10, 100, 1000, 10000]:\n",
        "      ProcessedImage.num_of_zeros_in_name_prefix -= 1\n",
        "    self._name = \"img_%s%s\" % (''.join(['0' for _ in range(0, self.num_of_zeros_in_name_prefix)]), self._counter)\n",
        "\n",
        "  def _init_te_data(self, data: bytes = None) -> None:\n",
        "    self._te_data = None\n",
        "    if data is None:\n",
        "      return\n",
        "    te_obj_name_prefix = f\"{self.name}_te\"\n",
        "    self._te_data = {\"name\": f\"{self.name}_te\", \"data\": data}\n",
        "\n",
        "  def _init_tr_data(self, data: list) -> None:\n",
        "    self._tr_data = []\n",
        "    if len(data) == 0:\n",
        "      return\n",
        "    for tr_obj_data in data:\n",
        "      self._tr_data.append({\"name\": f\"{self.name}_tr_{self._counted_tr}\", \"data\": tr_obj_data})\n",
        "      self._counted_tr += 1\n",
        "\n",
        "class UnprocessedImage:\n",
        "  images_counted = 0\n",
        "\n",
        "  def __init__(self, name: str, data: bytes):\n",
        "    self._data = data\n",
        "    self._name = name\n",
        "    self._update_counter()\n",
        "\n",
        "  @property\n",
        "  def counter(self) -> int:\n",
        "    return self._counter\n",
        "\n",
        "  @property\n",
        "  def data(self) -> str:\n",
        "    return self._data\n",
        "\n",
        "  @property\n",
        "  def name(self) -> str:\n",
        "    dot_idx = self._name.find('.')\n",
        "    if dot_idx != -1:\n",
        "      return self._name[:dot_idx]\n",
        "    return self._name\n",
        "\n",
        "  def _update_counter(self):\n",
        "    self._counter = UnprocessedImage.images_counted\n",
        "    UnprocessedImage.images_counted += 1"
      ],
      "metadata": {
        "id": "fwp2pvGCt43a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_bytes_to_image(data_bytes: bytes) -> Image:\n",
        "  return Image.open(BytesIO(data_bytes))\n",
        "\n",
        "def convert_image_to_bytes(image: Image) -> bytes:\n",
        "  result = BytesIO()\n",
        "  image.save(result, format=\"PNG\")\n",
        "  result.seek(0)\n",
        "  return result.getvalue()\n",
        "\n",
        "def convert_image_to_grayscale(image: Image) -> Image:\n",
        "  return image.convert(\"L\")"
      ],
      "metadata": {
        "id": "B2kYdHhBoCCE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BucketOfImages API"
      ],
      "metadata": {
        "id": "eTrOiLBiqq3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch_of_images(client: storage.Client, blob_idx_start: int, num_of_blobs: int) -> list:\n",
        "  bucket = client.get_bucket(BUCKETS[\"unpreprocessed\"][\"name\"])\n",
        "  counted = 0\n",
        "  result = []\n",
        "  for index,blob in enumerate(bucket.list_blobs()):\n",
        "    if index < blob_idx_start:\n",
        "      continue\n",
        "    if counted > num_of_blobs - 1:\n",
        "      break\n",
        "    counted += 1\n",
        "    result.append(UnprocessedImage(blob.name, blob.download_as_bytes()))\n",
        "  return result\n",
        "\n",
        "def get_batches_of_images(client: storage.Client, num_of_batches: int, blob_idx_start: int, num_of_blobs: int) -> list:\n",
        "  result = []\n",
        "  while num_of_batches > 0:\n",
        "    result.append(get_batch_of_images(client, blob_idx_start, num_of_blobs))\n",
        "    blob_idx_start += num_of_blobs\n",
        "    num_of_batches -= 1\n",
        "  return result\n",
        "\n",
        "def upload_blob(bucket: storage.Bucket, path: str, data: bytes) -> None:\n",
        "  blob = bucket.blob(path)\n",
        "  blob.upload_from_string(data, content_type=\"image/%s\" % MODEL_INPUT[\"type\"])\n",
        "\n",
        "def persist_processed_image(bucket: storage.Bucket, processed_image: ProcessedImage) -> None:\n",
        "  bucket_tr_name = BUCKETS[\"preprocessed\"][\"train\"][\"name\"]\n",
        "  for tr_obj in processed_image.tr_data:\n",
        "    upload_blob(bucket, \"%s/%s.png\" % (bucket_tr_name, tr_obj[\"name\"]), tr_obj[\"data\"])\n",
        "  if not processed_image.te_data is not None:\n",
        "    return\n",
        "  bucket_te_name = BUCKETS[\"preprocessed\"][\"test\"][\"name\"]\n",
        "  upload_blob(bucket, \"%s/%s.png\" % (bucket_te_name, processed_image.te_data[\"name\"]), processed_image.te_data[\"data\"])\n",
        "\n",
        "def persist_batch_of_images(client: storage.Client, processed_batch: list) -> None:\n",
        "  bucket = client.bucket(BUCKETS[\"preprocessed\"][\"name\"])\n",
        "  for processed_image in processed_batch:\n",
        "    persist_processed_image(bucket, processed_image)"
      ],
      "metadata": {
        "id": "3oEXyUGgqw0k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task API\n",
        "A `Task` in the context of this Notebook is a piece of logic that performs the following operations on a batch of images:\n",
        "\n",
        "*   Resize\n",
        "*   Conversion to Grayscale\n",
        "*   Crop\n",
        "*   Rotation\n",
        "\n",
        "The necessity for `Task` arises from [Global Interpreter Lock (GIL)](https://docs.python.org/3/glossary.html#term-global-interpreter-lock).  \n",
        "Python doesn't support parallelism as well as other counterparts. Hence, we will create a `Task` for each batch of unpreprocessed images.\n",
        "\n",
        "Module [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) will execute all of the tasks at the same time, thus bypassing GIL.\n",
        "\n",
        "Our primary limitation is `bottleneck` operations, such as authentication and client connection to Google Cloud buckets.  \n",
        "We will omit them from `Task` to avoid potentially complicated issues debugging that are unrelated to the project.  \n",
        "In return, execution will be more expensive in terms of consumed RAM and time. Which is a tradeoff that doesn't arise any objections for now."
      ],
      "metadata": {
        "id": "nqDcaMl97Z26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image_classic(image: Image, image_counter: int, is_tr: bool) -> ProcessedImage:\n",
        "  width, height = image.size\n",
        "  if width < MODEL_INPUT[\"width\"] or height < MODEL_INPUT[\"height\"]:\n",
        "    return None\n",
        "  if width > MODEL_INPUT[\"width\"] or height > MODEL_INPUT[\"height\"]:\n",
        "    image.thumbnail((MODEL_INPUT[\"width\"], MODEL_INPUT[\"height\"]))\n",
        "  if is_tr:\n",
        "      return ProcessedImage(image_counter, tr_data=[convert_image_to_bytes(image)])\n",
        "  return ProcessedImage(image_counter, te_data=convert_image_to_bytes(image))\n",
        "\n",
        "def crop_image_horizontally(image: Image) -> list:\n",
        "  result = []\n",
        "  width, height = image.size\n",
        "  width_ratio = width / MODEL_INPUT[\"width\"]\n",
        "  if width_ratio <= 1:\n",
        "    return [image]\n",
        "  for i in range(int(width_ratio) + 1):\n",
        "    left_x = i * MODEL_INPUT[\"width\"]\n",
        "    if left_x >= width:\n",
        "      break\n",
        "    right_x = (i + 1) * MODEL_INPUT[\"width\"]\n",
        "    if right_x > width:\n",
        "      right_x = width\n",
        "    result.append(image.crop((left_x, 0, right_x, height)))\n",
        "  return result\n",
        "\n",
        "def crop_image_vertically(image: Image) -> list:\n",
        "  result = []\n",
        "  width, height = image.size\n",
        "  height_ratio = height / MODEL_INPUT[\"height\"]\n",
        "  if height_ratio <= 1:\n",
        "    return [image]\n",
        "  for i in range(int(height_ratio) + 1):\n",
        "    bottom_y = i * MODEL_INPUT[\"height\"]\n",
        "    if bottom_y >= height:\n",
        "      break\n",
        "    upper_y = (i + 1) * MODEL_INPUT[\"height\"]\n",
        "    if upper_y > height:\n",
        "      upper_y = height\n",
        "    result.append(image.crop((0, bottom_y, width, upper_y)))\n",
        "  return result\n",
        "\n",
        "def frame_image_in_noise(image: Image) -> Image:\n",
        "  width, height = image.size\n",
        "  noise = np.random.normal(loc=MODEL_INPUT[\"noise_mean\"], scale=MODEL_INPUT[\"noise_std\"], size=(MODEL_INPUT[\"height\"], MODEL_INPUT[\"width\"], MODEL_INPUT[\"color_channels\"])).astype(np.uint8)\n",
        "  framed_image = Image.fromarray(noise)\n",
        "  top_left_x = (MODEL_INPUT[\"width\"] - width) // 2\n",
        "  top_left_y = (MODEL_INPUT[\"height\"] - height) // 2\n",
        "  framed_image.paste(image, (top_left_x, top_left_y))\n",
        "  return framed_image\n",
        "\n",
        "def process_image_noise_framed(image: Image, image_counter: int) -> ProcessedImage:\n",
        "  width, height = image.size\n",
        "  if width <= MODEL_INPUT[\"width\"] or height <= MODEL_INPUT[\"height\"]:\n",
        "    framed_image = frame_image_in_noise(image)\n",
        "    return ProcessedImage(image_counter, tr_data=[convert_image_to_bytes(framed_image)])\n",
        "  cropped_images = []\n",
        "  for vertically_cropped_image in crop_image_vertically(image):\n",
        "    for cropped_image in crop_image_horizontally(vertically_cropped_image):\n",
        "      cropped_images.append(cropped_image)\n",
        "  framed_images = [frame_image_in_noise(cropped_image) for cropped_image in cropped_images]\n",
        "  return ProcessedImage(image_counter, tr_data=[convert_image_to_bytes(framed_image) for framed_image in framed_images])\n",
        "\n",
        "def process_image(unprocessed_image: UnprocessedImage) -> ProcessedImage:\n",
        "  image = convert_bytes_to_image(unprocessed_image.data)\n",
        "  is_tr = unprocessed_image.counter % MODEL_INPUT[\"te_image_to_tr_images\"] == 0\n",
        "  if FLOW == \"classic\" or not is_tr:\n",
        "    return process_image_classic(image, unprocessed_image.counter, is_tr)\n",
        "  if FLOW == \"noise_framed\":\n",
        "    if is_tr:\n",
        "      return process_image_noise_framed(image, unprocessed_image.counter)\n",
        "    return process_image_classic(image, unprocessed_image.counter, is_tr)\n",
        "\n",
        "def preprocessing_task(output_queue: multiprocessing.Queue, batch_of_images: list = []) -> None:\n",
        "  if output_queue is None or len(batch_of_images) == 0:\n",
        "    output_queue.put(None)\n",
        "    return\n",
        "  results = []\n",
        "  for unprocessed_image in batch_of_images:\n",
        "    processed_image = process_image(unprocessed_image)\n",
        "    if processed_image is None:\n",
        "      continue\n",
        "    output_queue.put(processed_image)\n",
        "  output_queue.put(None)"
      ],
      "metadata": {
        "id": "KzgRf57e7bUK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Back to Table of Contents](#table-of-contents)"
      ],
      "metadata": {
        "id": "shZxaHdik-VR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Execution Path\n",
        "This is where we collect data for the preprocess tasks, run them and store their results."
      ],
      "metadata": {
        "id": "S0P5mz1rb6tA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h-MsQz5bhaj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_processed_image_idx = 0\n",
        "while True:\n",
        "  queue = multiprocessing.Queue()\n",
        "  processes = []\n",
        "  processed_batch = []\n",
        "  processes_outputs_left = 0\n",
        "  images_batches = get_batches_of_images(client, MACHINE[\"tasks_number\"], last_processed_image_idx, MACHINE[\"batch_size\"])\n",
        "  if len(images_batches) == 0:\n",
        "    break\n",
        "  for batch in images_batches:\n",
        "    processes.append(multiprocessing.Process(target=preprocessing_task, args=(queue, batch)))\n",
        "  processes_outputs_left = len(processes)\n",
        "  for process in processes:\n",
        "    process.start()\n",
        "  while processes_outputs_left > 0:\n",
        "    if queue.empty():\n",
        "      sleep(2)\n",
        "      continue\n",
        "    process_output = queue.get()\n",
        "    if process_output is None:\n",
        "      processes_outputs_left -= 1\n",
        "      continue\n",
        "    elif type(process_output) is ProcessedImage:\n",
        "      processed_batch.append(process_output)\n",
        "    else:\n",
        "      raise RuntimeError(\"Unexpected type for output of independent process: %s\" % type(process_output))\n",
        "  for process in processes:\n",
        "    process.join()\n",
        "  persist_batch_of_images(client, processed_batch)\n",
        "  last_processed_image_idx += MACHINE[\"tasks_number\"] * MACHINE[\"batch_size\"]"
      ],
      "metadata": {
        "id": "XgzdhtGme_4v"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Back to Table of Contents](#table-of-contents)"
      ],
      "metadata": {
        "id": "D26gbjoxe-q3"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}